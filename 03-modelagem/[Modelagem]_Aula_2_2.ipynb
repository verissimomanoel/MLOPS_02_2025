{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Identificação\n",
    "\n",
    "**Assunto:** Modelagem\n",
    "\n",
    "**Tutor:** Manoel Veríssimo dos Santos Neto"
   ],
   "metadata": {
    "id": "a_bguvWHyu4U"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Processamento de Linguagem Natural utilizando a biblioteca Hugging Face Transformers\n",
    "\n",
    "## 1- Objetivos de Aprendizagem\n",
    "\n",
    "Neste Notebook, vamos explorar exemplos reais de soluções para tarefas de Processamento de Linguagem Natural (NLP) utilizando a biblioteca Hugging Face Transformers. Nosso primeiro exemplo será o *fine-tuning* do modelo BERT para a análise de sentimentos em português. Em seguida, apresentaremos dois exemplos utilizando apenas modelos pré-treinados com os *pipelines* da Hugging Face. O segundo exemplo demonstrará o uso de* Named Entity Recognition* (NER), e o terceiro exemplo abordará a tradução automática. Estes exemplos práticos fornecerão uma visão abrangente de como aplicar técnicas avançadas de NLP para resolver problemas do mundo real.\n",
    "\n"
   ],
   "metadata": {
    "id": "gfevqw7Eyf76"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2- *Fine-Tuning* do BERT para Análise de Sentimentos\n",
    "\n",
    "Neste tópico, vamos realizar o *fine-tunin*g do modelo BERT para a tarefa de análise de sentimentos utilizando a biblioteca Hugging Face Transformers. Vamos usar um *dataset* de comentários sobre compras em português.\n",
    "\n",
    "**Importante:** Para o treinamento deve ser usada a rumtime com **GPU**.\n"
   ],
   "metadata": {
    "id": "OJA5knmZwYFH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1- Instalação das Dependências\n",
    "\n",
    "Vamos começar instalando as bibliotecas necessárias."
   ],
   "metadata": {
    "id": "ZW11YlHyzkX6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install torch\n",
    "!pip install evaluate"
   ],
   "metadata": {
    "id": "hvZsiZ_9xRbd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2- Configuração de variáveis globais\n",
    "\n",
    "Vamos configurar as variáveis de parâmetros para serem utilizadas durante o código."
   ],
   "metadata": {
    "id": "cyIq7wB3uRTl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"adalbertojunior/distilbert-portuguese-cased\"\n",
    "max_length= 512\n",
    "num_labels = 3\n",
    "batch_size = 28\n",
    "results_path = \"./results\"\n",
    "pretrained_path = \"./sentiment-analysis-bert-portuguese\""
   ],
   "metadata": {
    "id": "Ee6ne-tIuQj6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3- Importação das Bibliotecas\n",
    "\n",
    "Vamos importar as bibliotecas necessárias para carregar o *dataset*, tokenizar os textos, configurar o modelo BERT, e realizar o treinamento e avaliação."
   ],
   "metadata": {
    "id": "TO9J2RgG0Wga"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import wandb\n",
    "\n",
    "wandb.init(mode=\"disabled\")  # Desabilita o Wandb"
   ],
   "metadata": {
    "id": "r4NW3Bfk0Rwg"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4- Carregar o *Dataset*\n",
    "\n",
    "Vamos carregar um *dataset* de comentários de pedidos em português.\n",
    "Para este exemplo, vamos utilizar o *dataset* `verissimomanoel/olist_customers_review` da Hugging Face, que contém comentários rotulados para análise de sentimentos.\n",
    "No dataset já tem uma parte de treino `train` e outra de para teste `test`. Contudo precisamos dividir o treino mais uma vez para ter uma parte para validação `val` que será usada durante o treino."
   ],
   "metadata": {
    "id": "2-j83Sjz0h-a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Carrega o dataset que tem train e test\n",
    "dataset = load_dataset(\"verissimomanoel/olist_customers_review\", trust_remote_code=True)\n",
    "\n",
    "# Divide o treino em 80% e 20%, sendo os 80 para treino e os 20 para validação\n",
    "ds_train_split = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Monta o dataset com todas as partes train, test e val\n",
    "dataset = DatasetDict({\n",
    "    \"train\": ds_train_split[\"train\"],\n",
    "    \"test\": dataset[\"test\"],\n",
    "    \"val\": ds_train_split[\"test\"],\n",
    "})\n",
    "\n",
    "# Separa as partes do dataset\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "val_dataset = dataset['val']"
   ],
   "metadata": {
    "id": "L3ue_4LH0eVw"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5- Visualização do *Dataset*\n",
    "\n",
    "Vamos visualizar a estrutura do *dataset* carregado. Para tal, as sequências de código [6] e [7] abaixo mostrarão, na forma de gráfico, os conjuntos de dados para treino, teste e validação."
   ],
   "metadata": {
    "id": "BYfC-Ebb0tLc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(dataset)"
   ],
   "metadata": {
    "id": "eWJaKskM0oIc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def show_info_dataset(dataset, title):\n",
    "    # Converter o dataset para um DataFrame do pandas\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    # Contar as ocorrências na coluna 'label'\n",
    "    label_counts = df['label'].value_counts()\n",
    "\n",
    "    # Mapeamento dos labels para nomes\n",
    "    label_names = {0: 'Negativo', 1: 'Positivo', 2: 'Neutro'}\n",
    "\n",
    "    # Obter os nomes das labels\n",
    "    labels = [label_names[label] for label in label_counts.index]\n",
    "\n",
    "    # Definir as cores para cada label\n",
    "    colors = ['green', 'red', 'blue']\n",
    "\n",
    "    # Plotar o gráfico de barras\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(labels, label_counts, color=colors)\n",
    "\n",
    "    # Adicionar os totais em cima das barras\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), va='bottom')  # va: vertical alignment\n",
    "\n",
    "    # Configurar o título e os rótulos dos eixos\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Total')\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "6-2MGVJct4Oo"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "show_info_dataset(train_dataset, 'Distribuição por Classe - Treino')\n",
    "show_info_dataset(val_dataset, 'Distribuição por Classe - Validação')\n",
    "show_info_dataset(test_dataset, 'Distribuição por Classe - Teste')"
   ],
   "metadata": {
    "id": "R2C-42X2t5BH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.6- Tokenização do *Dataset*\n",
    "\n",
    "Vamos carregar o tokenizer do BERT e usá-lo para tokenizar os textos no *dataset.* Vamos usar o modelo `neuralmind/bert-base-portuguese-cased`, que é um BERT treinado em português.\n",
    "\n",
    "O comando train_dataset.shuffle().select(range(5000)) é responsaável pelo\n",
    "embaralhamento e seleção de amostras. Neste caso está selecionando as primeiras 5000 amostras do conjunto de dados embaralhado.\n",
    "\n",
    "O comando train_dataset.map(tokenize_function, batched=True) realiza a tokenização no conjunto de dados de treinamento (train_dataset) e usa a função map() para aplicar a função de tokenização a cada lote de amostras do conjunto de dados. Com batched=True, a função é aplicada em lotes de amostrase não em uma amostra por vez, isso torna o processo mais eficiente."
   ],
   "metadata": {
    "id": "MozQLcDL01Ir"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "# Reduz o tamanho dos datasets apenas para conseguir rodar no Google Colab por menos tempo, para rodar com o dataset completo só comentar as próximas 3 linhas\n",
    "train_dataset = train_dataset.shuffle().select(range(5000))\n",
    "test_dataset = test_dataset.shuffle().select(range(1000))\n",
    "val_dataset = test_dataset.shuffle().select(range(800))\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ],
   "metadata": {
    "id": "Lv0J4FuF0r19"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.7- Formatando o Dataset\n",
    "\n",
    "Vamos definir o formato dos datasets para que o *Trainer* da Hugging Face possa processá-los corretamente.\n",
    "\n",
    "*   **set_format(type='torch', columns=['input_ids', 'attention_mask', 'label']):** Este método converte os datasets para o formato do **PyTorch** (tensors) para que possam ser usados diretamente em redes neurais. A função especifica que apenas as colunas input_ids, attention_mask e label serão mantidas no formato final.\n",
    "\n",
    "  *   **input_ids:** Contém os identificadores numéricos que representam as palavras ou tokens da entrada.\n",
    "\n",
    "  *   **attention_mask:** Indica quais tokens são relevantes (1) e quais são padding (0), para que o modelo saiba onde prestar atenção.\n",
    "\n",
    "  *   **label:** São as etiquetas associadas a cada exemplo, que o modelo deve prever (por exemplo, para uma tarefa de classificação)."
   ],
   "metadata": {
    "id": "II6ToPVi6L3r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ],
   "metadata": {
    "id": "Vl1_Y6kc6NBB"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.8- Configuração do Modelo BERT\n",
    "\n",
    "Vamos configurar o modelo BERT para a tarefa de classificação de sequência. Neste caso, estamos utilizando a versão `neuralmind/bert-base-portuguese-cased` do modelo."
   ],
   "metadata": {
    "id": "h84dMysP07gu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_id, num_labels=num_labels)"
   ],
   "metadata": {
    "id": "BvrAz-oJ0zfI"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.9- Função de Avaliação\n",
    "\n",
    "Vamos definir uma função de avaliação para calcular a precisão do modelo durante a avaliação. Utilizaremos a métrica de precisão (`accuracy`) fornecida pela biblioteca `datasets`."
   ],
   "metadata": {
    "id": "dETjnEzR1D3z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Transforma os logits (saída do modelo) em previsões de classe, atribuindo a cada exemplo a classe com a\n",
    "    # maior probabilidade, ou seja, a que tem o maior valor de logit.\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=torch.tensor(labels))"
   ],
   "metadata": {
    "id": "ENyx3BvE1BK-"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.10- Configuração dos Argumentos de Treinamento\n",
    "\n",
    "Vamos definir os parâmetros de treinamento, incluindo a taxa de aprendizado, tamanho do batch, número de épocas, e a estratégia de avaliação."
   ],
   "metadata": {
    "id": "ptvexshA14Jq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=results_path,                  # Diretório de saída para os resultados\n",
    "    evaluation_strategy=\"epoch\",              # Estratégia de avaliação (avaliar a cada época)\n",
    "    learning_rate=3e-5,                       # Taxa de aprendizado\n",
    "    per_device_train_batch_size=batch_size,   # Tamanho do batch de treino\n",
    "    per_device_eval_batch_size=batch_size,    # Tamanho do batch de avaliação\n",
    "    num_train_epochs=3,                       # Número de épocas de treinamento\n",
    "    weight_decay=0.01,                        # Decaimento de peso\n",
    ")"
   ],
   "metadata": {
    "id": "6-eN47ng1GQw"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.11- Treinamento do Modelo\n",
    "\n",
    "Vamos criar um objeto `Trainer` com o modelo, dados de treino e validação, e os argumentos de treinamento definidos nos comando acima. Em seguida, vamos iniciar o treinamento do modelo."
   ],
   "metadata": {
    "id": "UKnYBv_y18vC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "id": "3u98LZ_T16Ql"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.12- Avaliação do Modelo - *Evaluate*\n",
    "\n",
    "Vamos avaliar o modelo no conjunto de teste ( com o comando trainer.evaluate) e exibir a precisão com a função show_info logo abaixo."
   ],
   "metadata": {
    "id": "wVZP3Vs_7zlI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"Acurácia no conjunto de teste: {results['eval_accuracy']}\")"
   ],
   "metadata": {
    "id": "pGMs6gqg2iPU"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.13- Avaliação do Modelo - *Predict*\n",
    "\n",
    "Vamos avaliar o modelo no conjunto de teste e exibir a matriz de confusão."
   ],
   "metadata": {
    "id": "mo6dKps6umDv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "raw_pred, _, _ = trainer.predict(test_dataset=test_dataset)"
   ],
   "metadata": {
    "id": "3e5RvhcWun1A"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def show_info(y_true, y_pred, title='Confusion matrix', cmap='Blues'):\n",
    "    target_names = ['Negativo', 'Positivo', 'Neutro']\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    disp.plot(ax=ax, xticks_rotation='vertical', cmap=plt.cm.Blues,values_format='g')\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "r59x4Cggupsz"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Pré-processar previsões brutas\n",
    "y_pred = np.argmax(raw_pred, axis=1)\n",
    "y_true = test_dataset[\"label\"]\n",
    "\n",
    "show_info(y_true, y_pred)"
   ],
   "metadata": {
    "id": "3-7yNnLXuqxx"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.14- Salvar o Modelo\n",
    "\n",
    "Vamos salvar o modelo treinado e o tokenizer para uso futuro."
   ],
   "metadata": {
    "id": "Zbnha-EL78sr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.save_pretrained(pretrained_path)\n",
    "tokenizer.save_pretrained(pretrained_path)"
   ],
   "metadata": {
    "id": "Treod7WH1EWh"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.15- Predição de Novos Exemplos\n",
    "\n",
    "Vamos definir uma função para prever o sentimento de novas frases usando o modelo treinado. Em seguida, vamos testar o modelo com uma nova frase e exibir o resultado."
   ],
   "metadata": {
    "id": "Fe0UgAUe8GlQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_sentiment(text):\n",
    "    # Certificar que o modelo e os inputs estão no mesmo dispositivo (CPU ou GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokeniza o texto, aplica padding e truncamento, converte para tensor PyTorch e move os dados para o dispositivo especificado.\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Executa o modelo com as entradas fornecidas, passando os tensores de input como argumentos para gerar as previsões ou saídas do modelo.\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Aplica a função softmax as saídas do modelo para converter os valores em probabilidades, normalizadas ao longo da última dimensão.\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Retorna a maior probabilidade\n",
    "    return probs.argmax().item()\n",
    "\n",
    "example_text = \"Eu adorei esse filme! Foi fantástico.\"\n",
    "predicted_label = predict_sentiment(example_text)\n",
    "sentiment = ['Negativo', 'Positivo', 'Neutro']\n",
    "print(f\"Sentimento previsto: {sentiment[predicted_label]}\")"
   ],
   "metadata": {
    "id": "AmHP2jYB8DcB"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.16- F1 *Score* - Importância em Datasets Desbalanceados\n",
    "\n",
    "O F1 *Score* é uma métrica usada para avaliar a performance de um modelo de classificação, especialmente quando lidamos com datasets desbalanceados. Para entender o F1 Score, é importante conhecer alguns conceitos básicos:\n",
    "\n",
    "1. **Acurácia (*Accuracy*)**: Métrica de avaliação utilizada para medir a proporção de previsões corretas em relação ao total de previsões feitas por um modelo de classificação. Ela é definida pela fórmula:\n",
    "    $$\n",
    "    \\text{Acurácia} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "    $$\n",
    "\n",
    "Onde:\n",
    "- **TP** (*True Positives*): Verdadeiros Positivos\n",
    "- **TN** (*True Negatives*): Verdadeiros Negativos\n",
    "- **FP** (*False Positives*): Falsos Positivos\n",
    "- **FN** (*False Negatives*): Falsos Negativos\n",
    "\n",
    "2. **Precisão (*Precision*)**: É a proporção de verdadeiros positivos (TP) entre todas as instâncias que o modelo previu como positivas. Em outras palavras, é a quantidade de previsões corretas de uma classe específica em relação ao total de previsões feitas para essa classe.\n",
    "   $$\n",
    "   \\text{Precisão} = \\frac{TP}{TP + FP}\n",
    "   $$\n",
    "   onde FP são os falsos positivos.\n",
    "\n",
    "3. **Revocação (*Recall*)**: É a proporção de verdadeiros positivos entre todas as instâncias que são realmente positivas. Ou seja, é a quantidade de previsões corretas de uma classe específica em relação ao total de instâncias reais dessa classe.\n",
    "   $$\n",
    "   \\text{Revocação} = \\frac{TP}{TP + FN}\n",
    "   $$\n",
    "   onde FN são os falsos negativos.\n",
    "\n",
    "4. **F1 Score**: É a média harmônica entre a Precisão e a Revocação. A média harmônica é usada aqui porque penaliza valores extremos, garantindo que o F1* Score* será baixo se um dos dois (Precisão ou Revocação) estiver baixo.\n",
    "   $$\n",
    "   \\text{F1 Score} = 2 \\times \\frac{\\text{Precisão} \\times \\text{Revocação}}{\\text{Precisão} + \\text{Revocação}}\n",
    "   $$\n",
    "\n",
    "### Por que usar o F1 Score em datasets desbalanceados?\n",
    "\n",
    "Em datasets desbalanceados, onde uma classe é muito mais frequente do que outra, métricas como a acurácia podem ser enganosas. Por exemplo, se temos 95% das instâncias de uma classe e apenas 5% de outra, um modelo que sempre prevê a classe majoritária terá alta acurácia, mas não será útil para detectar a classe minoritária.\n",
    "\n",
    "O F1 Score é importante porque leva em consideração tanto a Precisão quanto a Revocação. Em um cenário desbalanceado, isso ajuda a fornecer uma visão mais equilibrada da performance do modelo, destacando se ele é capaz de identificar a classe minoritária com precisão e frequência suficientes.\n",
    "\n",
    "Assim, o F1 Score é particularmente útil quando a prioridade é garantir que tanto a taxa de detecção dos positivos (revocação) quanto a qualidade das detecções positivas (precisão) são importantes, o que é frequentemente o caso em situações desbalanceadas."
   ],
   "metadata": {
    "id": "HzLgLltmu5Gc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.17- Exercícios *Fine-Tuning*\n",
    "\n",
    "\n",
    "1.   Alterar alguns parâmetros de treinamento, como: batch_size, learning rate e número de épocas. Avaliar qual o impacto negativo ou positivo na alteração desses parâmetros.\n",
    "2.   Utilizar alguma técnica de balanceamento de *dataset* e avaliar os resultados, Ex.: *Oversampling* e *Undersampling*\n",
    "\n",
    "**Importante:**\n",
    "\n",
    "*   Todas as alterações devem ser registradas no Wandb para que seja possível realizar comparações entre os experimentos.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "cmgLgoLyzKLA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3- BERT para NER\n",
    "\n",
    "Neste tópico, vamos demonstrar como usar a biblioteca Hugging Face Transformers e *pipelines* de um modelo já treinado para realizar *Named Entity Recognition *(NER) em textos em português. Utilizaremos um modelo pré-treinado adequado para a tarefa de NER."
   ],
   "metadata": {
    "id": "2ka8TyLD0Eun"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1- Configuração de variáveis globais\n",
    "\n",
    "Vamos configurar as variáveis de parâmetros para serem utilizadas durante o código."
   ],
   "metadata": {
    "id": "hQsqxzNZ2cBq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Seleciona a versão do modelo que será utilizada\n",
    "model_id = \"lfcc/bert-portuguese-ner\""
   ],
   "metadata": {
    "id": "5Tt9jZOW2dUL"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2- Importação das Bibliotecas\n",
    "\n",
    "Vamos importar as bibliotecas necessárias para carregar o pipeline de NER e o *dataset* em português."
   ],
   "metadata": {
    "id": "QhpLV3pq0_6J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer"
   ],
   "metadata": {
    "id": "i16vHS_v8KUr"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3- Carregar o Modelo BERT para NER\n",
    "\n",
    "Vamos carregar o *pipeline* de NER usando um modelo pré-treinado disponível na Hugging Face. Utilizaremos o modelo `xlm-roberta-base`, que é adequado para NER em português.\n",
    "\n",
    "Ao carregar o *pipeline* NER, usando o  aggregation_strategy=\"simple\" o modelo adotará a estratégia de agregaçao para unir tokens que pertençam a uma mesma entidade nomeada."
   ],
   "metadata": {
    "id": "PedrcaYN1FkY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Carregando o modelo e tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Carregando o pipeline de NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ],
   "metadata": {
    "id": "0d7LDxAJ1CzG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4- Extraindo Entidades de Texto\n",
    "\n",
    "Vamos utilizar o pipeline de NER para identificar entidades nomeadas em alguns exemplos de texto em português."
   ],
   "metadata": {
    "id": "ChW-E1B4299C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Definindo alguns exemplos de texto em português\n",
    "examples = [\n",
    "    \"Paulo viajou para o Estados Unidos.\",\n",
    "    \"Marie Curie foi uma cientista polonesa que realizou pesquisas pioneiras sobre radioatividade.\",\n",
    "    \"Fernando Henrique Cardoso foi o primeiro presidente eleito após a ditadura no Brasil.\",\n",
    "    \"Petrobras foi fundada em 3 de outubro de 1953\"\n",
    "]\n",
    "\n",
    "# Realizando NER nos exemplos de texto\n",
    "for example in examples:\n",
    "    ner_results = ner_pipeline(example)\n",
    "    print(f\"Texto: {example}\")\n",
    "    print(\"Entidades Nomeadas:\")\n",
    "    for entity in ner_results:\n",
    "        print(f\" - {entity['word']}: {entity['entity_group']} ({entity['score']*100:.2f}%)\")\n",
    "    print()"
   ],
   "metadata": {
    "id": "TdaCWW8M24w7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4- **Tradução** Automática usando Hugging Face Transformers\n",
    "\n",
    "Neste tópico, vamos demonstrar como usar a biblioteca Hugging Face Transformers e *pipelines* de um modelo já treinado para realizar a tradução automática de textos em português para o inglês. Utilizaremos um modelo pré-treinado adequado para a tarefa de tradução."
   ],
   "metadata": {
    "id": "wFEOSozf75KJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1- Importação das Bibliotecas\n",
    "\n",
    "Vamos importar as bibliotecas necessárias para carregar o pipeline de tradução."
   ],
   "metadata": {
    "id": "hGQoHTzC8CT3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline"
   ],
   "metadata": {
    "id": "5IEKNzDL7Vcl"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2- Carregar o Modelo de Tradução\n",
    "\n",
    "Vamos carregar o *pipeline* de tradução usando um modelo pré-treinado disponível na Hugging Face. Utilizaremos o modelo `Helsinki-NLP/opus-mt-pt-en`, que é adequado para tradução do português para o inglês."
   ],
   "metadata": {
    "id": "xEuOo40L8KPH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Carregando o pipeline de tradução\n",
    "translation_pipeline = pipeline(\"translation_en_to_pt\", model=\"Helsinki-NLP/opus-mt-tc-big-en-pt\")"
   ],
   "metadata": {
    "id": "-nsxw_xj8ILi"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3- Exemplos de Tradução\n",
    "\n",
    "Vamos utilizar o* pipeline* de tradução para traduzir alguns exemplos de texto em português para o inglês."
   ],
   "metadata": {
    "id": "2DYAI2on9HOG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Definindo alguns exemplos de texto em português\n",
    "examples = [\n",
    "    \"I love learning about natural language processing.\",\n",
    "    \"The BERT model was developed by Google AI Research.\",\n",
    "    \"Machine translation is a challenging and interesting task.\"\n",
    "]\n",
    "\n",
    "# Realizando a tradução dos exemplos de texto\n",
    "for example in examples:\n",
    "    translation = translation_pipeline(example)\n",
    "    print(f\"Texto original: {example}\")\n",
    "    print(f\"Tradução: {translation[0]['translation_text']}\\n\")"
   ],
   "metadata": {
    "id": "4C7IPrgl8Q01"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.4- Exercício - Desafio\n",
    "Para finalizar a seção do **BERT** do nosso curso fica o exercício desafio que será divido em três partes:\n",
    "\n",
    "\n",
    "1.   Utilizar o *dataset *`hate-speech-portuguese/hate_speech_portuguese` e dividi-lo em 3 partes `train`, `test` e `val`\n",
    "2.   Usar o *dataset* dividido e avaliar somente a parte do `val`, rodar a predição no modelo `adalbertojunior/distilbert-portuguese-cased` e avaliar a métrica **F1 Score** que deve ser calculada usando o `evaluate` (https://huggingface.co/docs/evaluate/v0.1.2/en/package_reference/loading_methods) do Hugging Face.\n",
    "3.  Realizar um *fine-tunning* para esse *dataset* e avaliar a métrica **F1 Score** comparando. Compare o resuldado desse modelo com o do passo anterior e veja qual ficou melhor.\n",
    "\n",
    "**Importante:**\n",
    "\n",
    "*   Todas as alterações devem ser registradas no Wandb para que seja possível realizar comparações entre os experimentos."
   ],
   "metadata": {
    "id": "IUU03r1dbyNm"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_udDxu-BbN9N"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
