# ServiÃ§o de Deploy de Modelos de Machine Learning

Este projeto implementa uma API robusta para servir modelos de machine learning em produÃ§Ã£o, construÃ­da com FastAPI e prÃ¡ticas modernas de desenvolvimento.

## ğŸ— Estrutura do Projeto

```
serving/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ endpoints.py      # Handlers das rotas da API
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py        # ConfiguraÃ§Ãµes da aplicaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ model_manager.py  # Gerenciamento do modelo ML
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ schemas.py        # Modelos de validaÃ§Ã£o de dados
â”œâ”€â”€ main.py                   # Ponto de entrada da aplicaÃ§Ã£o
â”œâ”€â”€ Dockerfile               # ConfiguraÃ§Ã£o para containerizaÃ§Ã£o
â””â”€â”€ pyproject.toml          # DependÃªncias do projeto
```

## ğŸš€ Funcionalidades Principais

- **API REST com FastAPI**: Interface moderna e de alta performance
- **Gerenciamento de Modelo**: Carregamento e serving eficiente de modelos ML
- **ValidaÃ§Ã£o de Dados**: Schemas Pydantic para garantir integridade dos dados
- **Logging**: Sistema de logs estruturados em JSON
- **ContainerizaÃ§Ã£o**: Docker multi-stage build para implantaÃ§Ã£o otimizada
- **Healthcheck**: Monitoramento da saÃºde do serviÃ§o
- **CORS**: Suporte configurado para requisiÃ§Ãµes cross-origin

## ğŸ›  Requisitos TÃ©cnicos

- Python 3.12+
- FastAPI
- Uvicorn
- Pydantic
- NumPy
- scikit-learn
- Docker (opcional)

## ğŸ”§ ConfiguraÃ§Ã£o e InstalaÃ§Ã£o

### InstalaÃ§Ã£o Local

```bash
# Crie e ative um ambiente virtual
python -m venv .venv
source .venv/bin/activate  # Linux/MacOS
# ou
.venv\Scripts\activate  # Windows

# Instale as dependÃªncias
pip install -e ".[all]"
```

### Usando Docker

```bash
# Construa a imagem
docker build -t ml-serving .

# Pra rodar o MLFlow (caso necessÃ¡rio)
mlflow server --host 0.0.0.0 --port 8080

# Execute o container
docker run -d \
  -p 8000:8000 \
  -e MODEL_NAME=random-forest \
  -e MODEL_VERSION_ALIAS=latest \
  --name ml-serving \
  ml-serving
```

## ğŸš€ Executando o ServiÃ§o

### Localmente

```bash
# Configure o modelo MLflow (opcional)
export MODEL_NAME="random-forest"
export MODEL_VERSION_ALIAS="latest"

# Execute o servidor
python main.py
# ou
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

## ğŸ“¡ Endpoints da API

### PrediÃ§Ã£o

- **URL**: `/api/v1/predict`
- **MÃ©todo**: POST
- **Corpo da RequisiÃ§Ã£o**:

```json
{
	"features": {
		"job": "management",
		"marital": "married",
		"education": "tertiary",
		"contact": "unknown",
		"housing": "yes",
		"loan": "no",
		"default": "no",
		"day": 5
	}
}
```

- **Resposta de Sucesso**:

```json
{
	"prediction": 1.0,
	"status": "success"
}
```

### Health Check

- **URL**: `/api/v1/health`
- **MÃ©todo**: GET
- **Resposta**:

```json
{
	"status": "healthy",
	"model_loaded": true
}
```

## ğŸ” Exemplo de Uso com cURL

```bash
# Health check
curl http://localhost:8000/api/v1/health

# Fazer uma prediÃ§Ã£o
curl -X POST "http://localhost:8000/api/v1/predict" \
  -H "Content-Type: application/json" \
  -d '{
        "features": {
         "job": "management",
         "marital": "married",
         "education": "tertiary",
         "contact": "unknown",
         "housing": "yes",
         "loan": "no",
         "default": "no",
         "day": 5
      }
  }'
curl -X POST "http://localhost:8000/api/v1/predict" \
  -H "Content-Type: application/json" \
  -d '{
        "features": {
        "job": "management",
         "marital": "single",
         "education": "tertiary",
         "contact": "unknown",
         "housing": "yes",
         "loan": "no",
         "default": "no",
         "day": 8
    }
  }'

```

## âš™ï¸ VariÃ¡veis de Ambiente

- `MODEL_NAME`: Nome do modelo no registro do MLflow
- `MODEL_VERSION_ALIAS`: VersÃ£o ou alias do modelo (ex: 'production', 'staging', '1')
- `PORT`: Porta do servidor (padrÃ£o: 8000)
- `HOST`: Host do servidor (padrÃ£o: 0.0.0.0)

## ğŸ” SeguranÃ§a

- ValidaÃ§Ã£o de entrada com Pydantic
- Logs estruturados para auditoria
- ContainerizaÃ§Ã£o segura com imagem slim
- Healthchecks para monitoramento

## ğŸ“¦ Estrutura do Modelo Manager

O `ModelManager` Ã© responsÃ¡vel por:

- Carregamento eficiente do modelo
- PrÃ©-processamento de dados de entrada
- ExecuÃ§Ã£o de prediÃ§Ãµes
- FormataÃ§Ã£o das respostas
- Tratamento de erros

## ğŸ”„ CI/CD e Deploy

### Docker

- Build multi-stage para imagens otimizada
- Healthcheck integrado
- ConfiguraÃ§Ã£o via variÃ¡veis de ambiente

## ğŸ“Š Monitoramento

- Endpoint de health check para status do serviÃ§o
- Logs estruturados em JSON
- InformaÃ§Ãµes sobre carregamento do modelo
- Tracking de erros de prediÃ§Ã£o

## ğŸ› ResoluÃ§Ã£o de Problemas

### Problemas Comuns

1. **Modelo nÃ£o carrega**:

   - Verifique se o caminho do modelo estÃ¡ correto
   - Confirme se o arquivo do modelo existe
   - Verifique as permissÃµes do arquivo

2. **Erro nas prediÃ§Ãµes**:

   - Confirme o formato dos dados de entrada
   - Verifique se todas as features necessÃ¡rias estÃ£o presentes
   - Consulte os logs para mais detalhes

3. **Servidor nÃ£o inicia**:
   - Verifique se a porta estÃ¡ disponÃ­vel
   - Confirme se todas as dependÃªncias estÃ£o instaladas
   - Verifique os logs de inicializaÃ§Ã£o

## ğŸ“ Notas de Desenvolvimento

- O serviÃ§o usa FastAPI para performance mÃ¡xima
- ImplementaÃ§Ã£o assÃ­ncrona para melhor escalabilidade
- Sistema de logging estruturado para facilitar debugging
- Design modular para fÃ¡cil manutenÃ§Ã£o e extensÃ£o
